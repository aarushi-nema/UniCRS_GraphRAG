{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing /home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl...\n",
      "Processed 1000 conversations...\n",
      "Processing complete. Successfully processed 1342 conversations.\n",
      "\n",
      "Sample of transformed conversation from /home/Nema/UniCRS_GraphRAG/data/redial/test_data.jsonl:\n",
      "{User: Hi I am looking for a movie like Super Troopers (2001) | Recommender: You should watch Police Academy  (1984) | User: Is that a great one? I have never seen it. I have seen American Pie  | User: I mean American Pie  (1999) | Recommender: Yes Police Academy  (1984) is very funny and so is Police Academy 2: Their First Assignment (1985) | User: It sounds like I need to check them out | Recommender: yes you will enjoy them | User: I appreciate your time. I will need to check those out. Are there any others you would recommend? | Recommender: yes Lethal Weapon (1987) | User: Thank you i will watch that too | Recommender: and also Beverly Hills Cop (1984) | User: Thanks for the suggestions. | Recommender: you are welcome | Recommender: and also 48 Hrs. (1982) | User: thanks goodbye}\n",
      "Transformation complete for /home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl\n",
      "\n",
      "Processing /home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl...\n",
      "Error processing conversation: 'list' object has no attribute 'items'\n",
      "Processed 1000 conversations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 conversations...\n",
      "Processed 3000 conversations...\n",
      "Processed 4000 conversations...\n",
      "Processed 5000 conversations...\n",
      "Processed 6000 conversations...\n",
      "Processed 7000 conversations...\n",
      "Processed 8000 conversations...\n",
      "Processed 9000 conversations...\n",
      "Processed 10000 conversations...\n",
      "Processing complete. Successfully processed 10005 conversations.\n",
      "Encountered 1 errors during processing.\n",
      "\n",
      "Sample of transformed conversation from /home/Nema/UniCRS_GraphRAG/data/redial/train_data.jsonl:\n",
      "{User: Hi there, how are you? I'm looking for movie recommendations | Recommender: I am doing okay. What kind of movies do you like? | User: I like animations like The Triplets of Belleville (2003) and Waking Life (2001) | User: I also enjoy Mary and Max (2009) | User: Anything artistic | Recommender: You might like The Boss Baby (2017) that was a good movie. | User: What's it about? | Recommender: It has Alec Baldwin it is about a baby that works for a company and gets adopted it is very funny | User: That seems like a nice comedy | User: Do you have any animated recommendations that are a bit more dramatic? Like A Scanner Darkly  (2006) for example | User: I like comedies but I prefer films with a little more depth | Recommender: That is a tough one but I will remember something | Recommender: Final Fantasy: The Spirits Within (2001) was a good one | User: Ooh that seems cool! Thanks for the input. I'm ready to submit if you are. | Recommender: It is animated, sci fi, and has action | Recommender: Glad I could help | User: Nice | User: Take care, cheers! | Recommender: bye}\n",
      "Transformation complete for /home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl\n"
     ]
    }
   ],
   "source": [
    "#NOT THIS\n",
    "import json\n",
    "\n",
    "def transform_messages(conversation):\n",
    "    \"\"\"Replace movie IDs with actual movie names.\"\"\"\n",
    "    movie_mentions = conversation.get(\"movieMentions\", {})\n",
    "    transformed_messages = []\n",
    "        \n",
    "    for message in conversation[\"messages\"]:\n",
    "        text = message[\"text\"]\n",
    "        # Replace all movie IDs with their names, skipping None values\n",
    "        for movie_id, movie_name in movie_mentions.items():\n",
    "            if movie_name is not None:  # Only replace if we have a valid movie name\n",
    "                text = text.replace(f\"@{movie_id}\", movie_name)\n",
    "            else:\n",
    "                # Keep the original ID if no name is available\n",
    "                text = text.replace(f\"@{movie_id}\", f\"movie_{movie_id}\")\n",
    "        \n",
    "        transformed_message = {\n",
    "            \"text\": text,\n",
    "            \"senderWorkerId\": message[\"senderWorkerId\"]\n",
    "        }\n",
    "        transformed_messages.append(transformed_message)\n",
    "    \n",
    "    return transformed_messages, conversation[\"initiatorWorkerId\"]\n",
    "\n",
    "def format_dialogue(messages, initiator_id):\n",
    "    \"\"\"Format messages into dialogue string.\"\"\"\n",
    "    dialogue = []\n",
    "    for msg in messages:\n",
    "        # Identify if sender is initiator or respondent\n",
    "        sender_type = \"User\" if msg[\"senderWorkerId\"] == initiator_id else \"Recommender\"\n",
    "        dialogue.append(f\"{sender_type}: {msg['text']}\")\n",
    "    \n",
    "    # Return dialogue wrapped in curly braces\n",
    "    return \"{\" + \" | \".join(dialogue) + \"}\"\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    \"\"\"Process the entire file from original format to final dialogue format.\"\"\"\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f, open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "            line_count = 0\n",
    "            error_count = 0\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        # Load original conversation\n",
    "                        conversation = json.loads(line)\n",
    "                        \n",
    "                        # Transform movie IDs to names\n",
    "                        transformed_messages, initiator_id = transform_messages(conversation)\n",
    "                        \n",
    "                        # Format into final dialogue\n",
    "                        final_dialogue = format_dialogue(transformed_messages, initiator_id)\n",
    "                        \n",
    "                        # Write to output file\n",
    "                        out_f.write(final_dialogue + '\\n')\n",
    "                        line_count += 1\n",
    "                        \n",
    "                        # Print progress every 1000 lines\n",
    "                        if line_count % 1000 == 0:\n",
    "                            print(f\"Processed {line_count} conversations...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        error_count += 1\n",
    "                        print(f\"Error processing conversation: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            print(f\"Processing complete. Successfully processed {line_count} conversations.\")\n",
    "            if error_count > 0:\n",
    "                print(f\"Encountered {error_count} errors during processing.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error opening files: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Define file paths for both files\n",
    "    file_pairs = [\n",
    "        {\n",
    "            'input': '/home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl',\n",
    "            'output': '/home/Nema/UniCRS_GraphRAG/data/redial/test_data.jsonl'\n",
    "        },\n",
    "        {\n",
    "            'input': '/home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl',\n",
    "            'output': '/home/Nema/UniCRS_GraphRAG/data/redial/train_data.jsonl'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process each file pair\n",
    "    for file_pair in file_pairs:\n",
    "        print(f\"\\nProcessing {file_pair['input']}...\")\n",
    "        process_file(file_pair['input'], file_pair['output'])\n",
    "        \n",
    "        # Print sample of final output\n",
    "        print(f\"\\nSample of transformed conversation from {file_pair['output']}:\")\n",
    "        try:\n",
    "            with open(file_pair['output'], 'r', encoding='utf-8') as f:\n",
    "                print(f.readline().strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading sample: {str(e)}\")\n",
    "        \n",
    "        print(f\"Transformation complete for {file_pair['input']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing /home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl...\n",
      "Processed 1000 conversations...\n",
      "Processing complete. Successfully processed 1342 conversations.\n",
      "\n",
      "Sample of transformed conversation from /home/Nema/UniCRS_GraphRAG/data/redial/test_data.csv:\n",
      "{'conversation_id': '20001', 'full_dialogue': 'User: Hi I am looking for a movie like Super Troopers (2001) | Recommender: You should watch Police Academy  (1984) | User: Is that a great one? I have never seen it. I have seen American Pie  | User: I mean American Pie  (1999) | Recommender: Yes Police Academy  (1984) is very funny and so is Police Academy 2: Their First Assignment (1985) | User: It sounds like I need to check them out | Recommender: yes you will enjoy them | User: I appreciate your time. I will need to check those out. Are there any others you would recommend? | Recommender: yes Lethal Weapon (1987) | User: Thank you i will watch that too | Recommender: and also Beverly Hills Cop (1984) | User: Thanks for the suggestions. | Recommender: you are welcome | Recommender: and also 48 Hrs. (1982) | User: thanks goodbye'}\n",
      "Transformation complete for /home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl\n",
      "\n",
      "Processing /home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl...\n",
      "Error processing conversation: 'list' object has no attribute 'items'\n",
      "Processed 1000 conversations...\n",
      "Processed 2000 conversations...\n",
      "Processed 3000 conversations...\n",
      "Processed 4000 conversations...\n",
      "Processed 5000 conversations...\n",
      "Processed 6000 conversations...\n",
      "Processed 7000 conversations...\n",
      "Processed 8000 conversations...\n",
      "Processed 9000 conversations...\n",
      "Processed 10000 conversations...\n",
      "Processing complete. Successfully processed 10005 conversations.\n",
      "Encountered 1 errors during processing.\n",
      "\n",
      "Sample of transformed conversation from /home/Nema/UniCRS_GraphRAG/data/redial/train_data.csv:\n",
      "{'conversation_id': '391', 'full_dialogue': \"User: Hi there, how are you? I'm looking for movie recommendations | Recommender: I am doing okay. What kind of movies do you like? | User: I like animations like The Triplets of Belleville (2003) and Waking Life (2001) | User: I also enjoy Mary and Max (2009) | User: Anything artistic | Recommender: You might like The Boss Baby (2017) that was a good movie. | User: What's it about? | Recommender: It has Alec Baldwin it is about a baby that works for a company and gets adopted it is very funny | User: That seems like a nice comedy | User: Do you have any animated recommendations that are a bit more dramatic? Like A Scanner Darkly  (2006) for example | User: I like comedies but I prefer films with a little more depth | Recommender: That is a tough one but I will remember something | Recommender: Final Fantasy: The Spirits Within (2001) was a good one | User: Ooh that seems cool! Thanks for the input. I'm ready to submit if you are. | Recommender: It is animated, sci fi, and has action | Recommender: Glad I could help | User: Nice | User: Take care, cheers! | Recommender: bye\"}\n",
      "Transformation complete for /home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl\n"
     ]
    }
   ],
   "source": [
    "# NOT THIS\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def transform_messages(conversation):\n",
    "    \"\"\"Replace movie IDs with actual movie names.\"\"\"\n",
    "    movie_mentions = conversation.get(\"movieMentions\", {})\n",
    "    transformed_messages = []\n",
    "\n",
    "    for message in conversation[\"messages\"]:\n",
    "        text = message[\"text\"]\n",
    "        # Replace all movie IDs with their names, skipping None values\n",
    "        for movie_id, movie_name in movie_mentions.items():\n",
    "            if movie_name is not None:  # Only replace if we have a valid movie name\n",
    "                text = text.replace(f\"@{movie_id}\", movie_name)\n",
    "            else:\n",
    "                # Keep the original ID if no name is available\n",
    "                text = text.replace(f\"@{movie_id}\", f\"movie_{movie_id}\")\n",
    "\n",
    "        transformed_message = {\n",
    "            \"text\": text,\n",
    "            \"senderWorkerId\": message[\"senderWorkerId\"]\n",
    "        }\n",
    "        transformed_messages.append(transformed_message)\n",
    "\n",
    "    return transformed_messages, conversation[\"initiatorWorkerId\"]\n",
    "\n",
    "def format_dialogue(messages, initiator_id):\n",
    "    \"\"\"Format messages into dialogue string.\"\"\"\n",
    "    dialogue = []\n",
    "    for msg in messages:\n",
    "        # Identify if sender is initiator or respondent\n",
    "        sender_type = \"User\" if msg[\"senderWorkerId\"] == initiator_id else \"Recommender\"\n",
    "        dialogue.append(f\"{sender_type}: {msg['text']}\")\n",
    "\n",
    "    # Return dialogue as a single string\n",
    "    return \" | \".join(dialogue)\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    \"\"\"Process the file and output as CSV.\"\"\"\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f, open(output_path, 'w', encoding='utf-8', newline='') as out_f:\n",
    "            csv_writer = csv.DictWriter(out_f, fieldnames=[\"conversation_id\", \"full_dialogue\"])\n",
    "            csv_writer.writeheader()\n",
    "\n",
    "            line_count = 0\n",
    "            error_count = 0\n",
    "\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        # Load original conversation\n",
    "                        conversation = json.loads(line)\n",
    "\n",
    "                        # Transform movie IDs to names\n",
    "                        transformed_messages, initiator_id = transform_messages(conversation)\n",
    "\n",
    "                        # Format into final dialogue\n",
    "                        final_dialogue = format_dialogue(transformed_messages, initiator_id)\n",
    "\n",
    "                        # Write to CSV\n",
    "                        csv_writer.writerow({\n",
    "                            \"conversation_id\": conversation.get(\"conversationId\", \"unknown\"),\n",
    "                            \"full_dialogue\": final_dialogue\n",
    "                        })\n",
    "\n",
    "                        line_count += 1\n",
    "\n",
    "                        # Print progress every 1000 lines\n",
    "                        if line_count % 1000 == 0:\n",
    "                            print(f\"Processed {line_count} conversations...\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        error_count += 1\n",
    "                        print(f\"Error processing conversation: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "            print(f\"Processing complete. Successfully processed {line_count} conversations.\")\n",
    "            if error_count > 0:\n",
    "                print(f\"Encountered {error_count} errors during processing.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening files: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Define file paths for both files\n",
    "    file_pairs = [\n",
    "        {\n",
    "            'input': '/home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl',\n",
    "            'output': '/home/Nema/UniCRS_GraphRAG/data/redial/test_data.csv'\n",
    "        },\n",
    "        {\n",
    "            'input': '/home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl',\n",
    "            'output': '/home/Nema/UniCRS_GraphRAG/data/redial/train_data.csv'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Process each file pair\n",
    "    for file_pair in file_pairs:\n",
    "        print(f\"\\nProcessing {file_pair['input']}...\")\n",
    "        process_file(file_pair['input'], file_pair['output'])\n",
    "\n",
    "        # Print sample of final output\n",
    "        print(f\"\\nSample of transformed conversation from {file_pair['output']}:\")\n",
    "        try:\n",
    "            with open(file_pair['output'], 'r', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                print(next(reader))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading sample: {str(e)}\")\n",
    "\n",
    "        print(f\"Transformation complete for {file_pair['input']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing /home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl...\n",
      "Processed 1000 conversations...\n",
      "Processing complete. Successfully processed 1342 conversations.\n",
      "\n",
      "Sample of transformed conversation from /home/Nema/UniCRS_GraphRAG/data/redial/test_data.txt:\n",
      "User: Hi I am looking for a movie like Super Troopers (2001) | Recommender: You should watch Police Academy  (1984) | User: Is that a great one? I have never seen it. I have seen American Pie  | User: I mean American Pie  (1999) | Recommender: Yes Police Academy  (1984) is very funny and so is Police Academy 2: Their First Assignment (1985) | User: It sounds like I need to check them out | Recommender: yes you will enjoy them | User: I appreciate your time. I will need to check those out. Are there any others you would recommend? | Recommender: yes Lethal Weapon (1987) | User: Thank you i will watch that too | Recommender: and also Beverly Hills Cop (1984) | User: Thanks for the suggestions. | Recommender: you are welcome | Recommender: and also 48 Hrs. (1982) | User: thanks goodbye\n",
      "Transformation complete for /home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl\n",
      "\n",
      "Processing /home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl...\n",
      "Error processing conversation: 'list' object has no attribute 'items'\n",
      "Processed 1000 conversations...\n",
      "Processed 2000 conversations...\n",
      "Processed 3000 conversations...\n",
      "Processed 4000 conversations...\n",
      "Processed 5000 conversations...\n",
      "Processed 6000 conversations...\n",
      "Processed 7000 conversations...\n",
      "Processed 8000 conversations...\n",
      "Processed 9000 conversations...\n",
      "Processed 10000 conversations...\n",
      "Processing complete. Successfully processed 10005 conversations.\n",
      "Encountered 1 errors during processing.\n",
      "\n",
      "Sample of transformed conversation from /home/Nema/UniCRS_GraphRAG/data/redial/train_data.txt:\n",
      "User: Hi there, how are you? I'm looking for movie recommendations | Recommender: I am doing okay. What kind of movies do you like? | User: I like animations like The Triplets of Belleville (2003) and Waking Life (2001) | User: I also enjoy Mary and Max (2009) | User: Anything artistic | Recommender: You might like The Boss Baby (2017) that was a good movie. | User: What's it about? | Recommender: It has Alec Baldwin it is about a baby that works for a company and gets adopted it is very funny | User: That seems like a nice comedy | User: Do you have any animated recommendations that are a bit more dramatic? Like A Scanner Darkly  (2006) for example | User: I like comedies but I prefer films with a little more depth | Recommender: That is a tough one but I will remember something | Recommender: Final Fantasy: The Spirits Within (2001) was a good one | User: Ooh that seems cool! Thanks for the input. I'm ready to submit if you are. | Recommender: It is animated, sci fi, and has action | Recommender: Glad I could help | User: Nice | User: Take care, cheers! | Recommender: bye\n",
      "Transformation complete for /home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl\n"
     ]
    }
   ],
   "source": [
    "# THIS\n",
    "import json\n",
    "\n",
    "def transform_messages(conversation):\n",
    "    \"\"\"Replace movie IDs with actual movie names.\"\"\"\n",
    "    movie_mentions = conversation.get(\"movieMentions\", {})\n",
    "    transformed_messages = []\n",
    "    \n",
    "    for message in conversation[\"messages\"]:\n",
    "        text = message[\"text\"]\n",
    "        # Replace all movie IDs with their names, skipping None values\n",
    "        for movie_id, movie_name in movie_mentions.items():\n",
    "            if movie_name is not None:  # Only replace if we have a valid movie name\n",
    "                text = text.replace(f\"@{movie_id}\", movie_name)\n",
    "            else:\n",
    "                # Keep the original ID if no name is available\n",
    "                text = text.replace(f\"@{movie_id}\", f\"movie_{movie_id}\")\n",
    "        \n",
    "        transformed_message = {\n",
    "            \"text\": text,\n",
    "            \"senderWorkerId\": message[\"senderWorkerId\"]\n",
    "        }\n",
    "        transformed_messages.append(transformed_message)\n",
    "    \n",
    "    return transformed_messages, conversation[\"initiatorWorkerId\"]\n",
    "\n",
    "def format_dialogue(messages, initiator_id):\n",
    "    \"\"\"Format messages into dialogue string.\"\"\"\n",
    "    dialogue = []\n",
    "    for msg in messages:\n",
    "        # Identify if sender is initiator or respondent\n",
    "        sender_type = \"User\" if msg[\"senderWorkerId\"] == initiator_id else \"Recommender\"\n",
    "        dialogue.append(f\"{sender_type}: {msg['text']}\")\n",
    "    \n",
    "    # Return dialogue without curly braces\n",
    "    return \" | \".join(dialogue)\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    \"\"\"Process the entire file from original format to final dialogue format.\"\"\"\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f, open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "            line_count = 0\n",
    "            error_count = 0\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        # Load original conversation\n",
    "                        conversation = json.loads(line)\n",
    "                        \n",
    "                        # Transform movie IDs to names\n",
    "                        transformed_messages, initiator_id = transform_messages(conversation)\n",
    "                        \n",
    "                        # Format into final dialogue\n",
    "                        final_dialogue = format_dialogue(transformed_messages, initiator_id)\n",
    "                        \n",
    "                        # Write to output file\n",
    "                        out_f.write(final_dialogue + '\\n')\n",
    "                        line_count += 1\n",
    "                        \n",
    "                        # Print progress every 1000 lines\n",
    "                        if line_count % 1000 == 0:\n",
    "                            print(f\"Processed {line_count} conversations...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        error_count += 1\n",
    "                        print(f\"Error processing conversation: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            print(f\"Processing complete. Successfully processed {line_count} conversations.\")\n",
    "            if error_count > 0:\n",
    "                print(f\"Encountered {error_count} errors during processing.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error opening files: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Define file paths for both files with .txt extension\n",
    "    file_pairs = [\n",
    "        {\n",
    "            'input': '/home/Nema/UniCRS_GraphRAG/data/redial/test_data_raw.jsonl',\n",
    "            'output': '/home/Nema/UniCRS_GraphRAG/data/redial/test_data.txt'\n",
    "        },\n",
    "        {\n",
    "            'input': '/home/Nema/UniCRS_GraphRAG/data/redial/train_data_raw.jsonl',\n",
    "            'output': '/home/Nema/UniCRS_GraphRAG/data/redial/train_data.txt'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process each file pair\n",
    "    for file_pair in file_pairs:\n",
    "        print(f\"\\nProcessing {file_pair['input']}...\")\n",
    "        process_file(file_pair['input'], file_pair['output'])\n",
    "        \n",
    "        # Print sample of final output\n",
    "        print(f\"\\nSample of transformed conversation from {file_pair['output']}:\")\n",
    "        try:\n",
    "            with open(file_pair['output'], 'r', encoding='utf-8') as f:\n",
    "                print(f.readline().strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading sample: {str(e)}\")\n",
    "        \n",
    "        print(f\"Transformation complete for {file_pair['input']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing /home/Nema/UniCRS_GraphRAG/data/redial/test_data.txt...\n",
      "Processed 1000 lines...\n",
      "\n",
      "Token Statistics:\n",
      "Total lines processed: 1356\n",
      "Total tokens: 218651\n",
      "Average tokens per line: 161.25\n",
      "Median tokens per line: 150.0\n",
      "Minimum tokens in a line: 1\n",
      "Maximum tokens in a line: 465\n",
      "\n",
      "Analyzing /home/Nema/UniCRS_GraphRAG/data/redial/train_data.txt...\n",
      "Processed 1000 lines...\n",
      "Processed 2000 lines...\n",
      "Processed 3000 lines...\n",
      "Processed 4000 lines...\n",
      "Processed 5000 lines...\n",
      "Processed 6000 lines...\n",
      "Processed 7000 lines...\n",
      "Processed 8000 lines...\n",
      "Processed 9000 lines...\n",
      "Processed 10000 lines...\n",
      "\n",
      "Token Statistics:\n",
      "Total lines processed: 10125\n",
      "Total tokens: 1738489\n",
      "Average tokens per line: 171.70\n",
      "Median tokens per line: 163\n",
      "Minimum tokens in a line: 1\n",
      "Maximum tokens in a line: 1189\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Simple whitespace-based tokenization.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "def analyze_file(file_path):\n",
    "    \"\"\"Read file and analyze token counts.\"\"\"\n",
    "    token_counts = []\n",
    "    total_tokens = 0\n",
    "    line_count = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    tokens = count_tokens(line.strip())\n",
    "                    token_counts.append(tokens)\n",
    "                    total_tokens += tokens\n",
    "                    line_count += 1\n",
    "                    \n",
    "                    # Print progress every 1000 lines\n",
    "                    if line_count % 1000 == 0:\n",
    "                        print(f\"Processed {line_count} lines...\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        avg_tokens = total_tokens / line_count if line_count > 0 else 0\n",
    "        median_tokens = statistics.median(token_counts) if token_counts else 0\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nToken Statistics:\")\n",
    "        print(f\"Total lines processed: {line_count}\")\n",
    "        print(f\"Total tokens: {total_tokens}\")\n",
    "        print(f\"Average tokens per line: {avg_tokens:.2f}\")\n",
    "        print(f\"Median tokens per line: {median_tokens}\")\n",
    "        print(f\"Minimum tokens in a line: {min(token_counts)}\")\n",
    "        print(f\"Maximum tokens in a line: {max(token_counts)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Define file paths\n",
    "    files = [\n",
    "        '/home/Nema/UniCRS_GraphRAG/data/redial/test_data.txt',\n",
    "        '/home/Nema/UniCRS_GraphRAG/data/redial/train_data.txt'\n",
    "    ]\n",
    "    \n",
    "    # Process each file\n",
    "    for file_path in files:\n",
    "        print(f\"\\nAnalyzing {file_path}...\")\n",
    "        analyze_file(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
